{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Patient Trajectories\n",
    "\n",
    "This file queries the MIMIC database and generates patient trajectories - ordered lists of tokenised clinical events and associated target endpoints that can be fed into prediction models\n",
    "\n",
    "Notes:\n",
    "\n",
    "1. These scripts work largely on parquet files directly for simplicity, except for vital signs, where loading into memory is prohibitive, so you must run a glue crawler to get processed data added to the glue catalogue to proceed with vital signs processing.\n",
    "\n",
    "2. In order to avoid label leakage into embedding generation, ICU admissions and discharges are tokenized separately to the rest of the patient trajectory\n",
    "\n",
    "3. We generate two versions of the patient trajectory - one based in clinical and administrative events (pathology, prescriptions, procedures, historical diagnoses, ventilation & patient demography) and one based in the trajectory of charted vital signs observations.\n",
    "\n",
    "4. For non-discrete events such as numeric pathology results, procedure durations, ventilation durations or numeric chart observations, we generate decile or quartile information to use as discrete labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyathena import connect\n",
    "from pyathena.util import as_pandas\n",
    "from pyathena.util import to_sql\n",
    "import pandas as pd\n",
    "import awswrangler as wr\n",
    "import sys,os,os.path, datetime, os, boto3, pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from itertools import product, repeat\n",
    "from collections import Counter\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "from utils.connections import connection, cursor, gluedatabase, processed_db, upload_file, processed_data_bucket, download_file, get_or_create_bucket, account_id, get_s3_keys_as_generator\n",
    "from utils.datagen import PROCESSED_DATAPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(tf.__version__[0]=='2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distinct_lab_events(RELOAD=False):    \n",
    "    \n",
    "    # We select all distinct test types from the lab events list \n",
    "    \n",
    "    if RELOAD:\n",
    "        wr.s3.delete_objects(path=f's3://{processed_data_bucket}/distinct_path')\n",
    "        query=f'select distinct(itemid) from {gluedatabase}.labevents;'\n",
    "        item_ids = pd.read_sql(query, connection)\n",
    "        wr.s3.to_parquet(df=item_ids, \n",
    "                         database=gluedatabase, \n",
    "                         table='distinct_path',\n",
    "                         dataset=True,\n",
    "                         path=f's3://{processed_data_bucket}/distinct_path')\n",
    "    else:\n",
    "        item_ids = wr.s3.read_parquet(path=f's3://{processed_data_bucket}/distinct_path')\n",
    "    return item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distinct_chart_events(RELOAD=False):    \n",
    "    \n",
    "    # We select all distinct observation types from the chart events list \n",
    "    \n",
    "    if RELOAD:\n",
    "        wr.s3.delete_objects(path=f's3://{processed_data_bucket}/distinct_chart')\n",
    "        query=f'select distinct(itemid) from {gluedatabase}.chartevents;'\n",
    "        item_ids = pd.read_sql(query, connection)\n",
    "        wr.s3.to_parquet(df=item_ids, \n",
    "                         database=gluedatabase, \n",
    "                         table='distinct_chart',\n",
    "                         dataset=True,\n",
    "                         path=f's3://{processed_data_bucket}/distinct_chart')\n",
    "    else:\n",
    "        item_ids = wr.s3.read_parquet(path=f's3://{processed_data_bucket}/distinct_chart')\n",
    "    return item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_tokens(item_ids, RELOAD=False):\n",
    "    \n",
    "    # Load all distinct results per event type and return relevant decile information\n",
    "    \n",
    "    if RELOAD:\n",
    "        wr.s3.delete_objects(path=f's3://{processed_data_bucket}/path_tokens')\n",
    "        keys = []\n",
    "    else:\n",
    "        try:\n",
    "            keys = list(wr.s3.read_parquet_table(database=gluedatabase, table='path_tokens').itemid.unique())\n",
    "        except EntityNotFoundException:\n",
    "            keys = []\n",
    "    found_keys = [i for i in item_ids.itemid if i in keys]\n",
    "    to_find_keys = [i for i in item_ids.itemid if not i in keys]\n",
    "    print(f'loading {len(found_keys)} keys from file and {len(to_find_keys)} keys from db')\n",
    "    \n",
    "    unit_mapper = {'mg/24hr':('mg/24hours', 1), \n",
    "                   'ug/dl':('ng/ml', 10), \n",
    "                   '#/ul':('#/cu mm', 1), \n",
    "                   'ug/l':('ng/ml', 1), \n",
    "                   'mosm/kg':('mosm/l', 1), \n",
    "                   'mg/l':('mg/dl', 0.1), \n",
    "                   'miu/ml':('miu/l', 1000), \n",
    "                   'iu/ml':('i.u.', 1), \n",
    "                   'sec':('seconds', 1), \n",
    "                   'eu/dl':('mg/dl', 0.037427), \n",
    "                   'uiu/ml':('uu/ml', 1)}\n",
    "\n",
    "    for i, item in enumerate(to_find_keys):\n",
    "        # for all test types that have numeric data, we select all test results for that given test type\n",
    "        query=f'select subject_id, hadm_id, itemid, charttime, valuenum, valueuom from {gluedatabase}.labevents where itemid={item};'\n",
    "        item_range = pd.read_sql(query, connection)\n",
    "        item_range = item_range.rename(columns={'valuenum': 'val', 'valueuom': 'units', 'charttime': 'time'})\n",
    "        item_range = item_range.dropna(subset=['hadm_id'])\n",
    "        item_range = item_range.astype({'subject_id': 'int64', \n",
    "                                        'hadm_id': 'int64', \n",
    "                                        'itemid': 'int64', \n",
    "                                        'val': 'double', \n",
    "                                        'units': 'string'})\n",
    "        item_range.units = item_range.units.str.lower()\n",
    "        try:\n",
    "            unit_count = item_range.units.value_counts()\n",
    "            if len(unit_count) > 1:\n",
    "                for unit in unit_count.index:\n",
    "                    if unit in unit_mapper.keys():\n",
    "                        multiplier = unit_mapper[unit][1]\n",
    "                        replacement_unit = unit_mapper[unit][0]\n",
    "                        item_range.loc[item_range.units == unit, 'val'] *= multiplier\n",
    "                        item_range.loc[item_range.units == unit, 'units'] = replacement_unit\n",
    "            item_range['decile'] = pd.qcut(item_range.val, q=10, labels=list(range(10)))        \n",
    "            item_range['label'] = item_range.itemid.astype(str) + '_' + item_range.decile.astype(str)\n",
    "            item_range = item_range.rename(columns={})\n",
    "        except:         \n",
    "            # will fail if non-numeric so no deciles for this selection - just include that \n",
    "            # this non-numeric test was ordered (presumably still has some signal value even \n",
    "            # without the result value, as it can be a proxy for clinician's intuition/which \n",
    "            # variables were viewed as in-scope)\n",
    "            item_range['label'] = item_range.itemid.astype(str)\n",
    "            \n",
    "        # for every token type, we retain the label only if there are more than 20 instances\n",
    "        # which means for numeric tests, we do not include them in the trajectory if there are \n",
    "        # fewer than 200 valid numeric results in the database\n",
    "        retain_labels = list(item_range.label.value_counts()[item_range.label.value_counts()>20].index)\n",
    "        d = item_range[item_range.label.isin(retain_labels)]\n",
    "        \n",
    "        if len(d) > 0:\n",
    "            wr.s3.to_parquet(df=d[['subject_id', 'hadm_id', 'time', 'label', 'itemid']], \n",
    "                             database=gluedatabase, \n",
    "                             table='path_tokens',\n",
    "                             dataset=True,\n",
    "                             partition_cols=['itemid'],\n",
    "                             path=f's3://{processed_data_bucket}/path_tokens')\n",
    "        else:\n",
    "            print(f'empty dataframe for itemid={item}')\n",
    "        if i % 20 == 0:\n",
    "            print(f'{i} of {len(to_find_keys)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vs_to_process():   \n",
    "    \n",
    "    # Quantity of vital signs data is very large so this function returns the list \n",
    "    # of chart-event types still requiring pre-processing.  We only include chart\n",
    "    # obs for the first 6 hours of the ICU admission.\n",
    "    \n",
    "    query = f'select distinct(itemid), count(*) from {processed_db}.vs_windows group by itemid;'\n",
    "    processed_keys = pd.read_sql(query, connection)\n",
    "\n",
    "    query = f'select distinct(itemid), count(*) FROM {gluedatabase}.icustays i INNER JOIN {gluedatabase}.chartevents c '\\\n",
    "            f'ON i.icustay_id = c.icustay_id WHERE '\n",
    "\n",
    "    condition1 = f'((c.charttime > c.storetime) AND (c.charttime > i.intime) AND ((to_unixtime(c.charttime) - to_unixtime(i.intime))/3600 < 6))'\n",
    "    condition2 = f'((c.storetime > c.charttime) AND (c.storetime > i.intime) AND ((to_unixtime(c.storetime) - to_unixtime(i.intime))/3600 < 6))'  \n",
    "\n",
    "    vs_window1 = pd.read_sql(f'{query} {condition1} group by itemid', connection)\n",
    "    vs_window2 = pd.read_sql(f'{query} {condition2} group by itemid', connection)\n",
    "\n",
    "    processed_keys.itemid = processed_keys.itemid.astype(int)\n",
    "\n",
    "    to_process_keys = pd.merge(vs_window1, vs_window2, how='outer', on='itemid')\n",
    "    to_process_keys.columns = ['itemid', 'query1', 'query2']\n",
    "    to_process_keys = to_process_keys.fillna(0)\n",
    "\n",
    "    processing_diff = pd.merge(to_process_keys, processed_keys, how='left', on='itemid')\n",
    "    processing_diff.columns = ['itemid', 'query1', 'query2', 'done']\n",
    "\n",
    "    processing_diff['todo'] = processing_diff.query1 + processing_diff.query2\n",
    "    processing_diff = processing_diff.fillna(0)\n",
    "    print(f'number of v.s. types still to process: {len(processing_diff[processing_diff.done < processing_diff.todo])}')\n",
    "    return list(processing_diff[processing_diff.done < processing_diff.todo].itemid)\n",
    "\n",
    "\n",
    "def make_vital_signs_windows(todo_items):\n",
    "    \n",
    "    # this function filters the vital signs data so that it only includes data captured within \n",
    "    # the first 6 hours of each ICU admission, and also partitions the data by itemid to speed up\n",
    "    # tokenization steps.\n",
    "    \n",
    "    query = f'SELECT i.icustay_id, i.intime, i.outtime, c.itemid, c.charttime, c.storetime, c.value, '\\\n",
    "            f'c.valuenum, c.valueuom FROM {gluedatabase}.icustays i INNER JOIN {gluedatabase}.chartevents c '\\\n",
    "            f'ON i.icustay_id = c.icustay_id WHERE '\n",
    "\n",
    "    # chart time and store time are inconsistent, so we select the latest time as the time of \n",
    "    # availability in the record to ensure it represents only data that we can be confident\n",
    "    # was without question available for real-time prediction\n",
    "    \n",
    "    condition1 = f'((c.charttime > c.storetime) AND (c.charttime > i.intime) AND ((to_unixtime(c.charttime) - to_unixtime(i.intime))/3600 < 6))'\n",
    "    condition2 = f'((c.storetime > c.charttime) AND (c.storetime > i.intime) AND ((to_unixtime(c.storetime) - to_unixtime(i.intime))/3600 < 6))'  \n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for condition, col in zip([condition1, condition2], ['charttime', 'storetime']):\n",
    "        print(col)\n",
    "        for item in todo_items:\n",
    "            i += 1\n",
    "            vs_window = pd.read_sql(f'{query} {condition} and itemid = {item}', connection, chunksize=5000)\n",
    "            for vs in vs_window:\n",
    "                vs = vs.rename(columns={col: 'time'})\n",
    "                vs.valuenum = vs.valuenum.astype(float)\n",
    "                vs.value = vs.value.astype(str)\n",
    "                vs.valueuom = vs.valueuom.astype(str)\n",
    "                wr.s3.to_parquet(df=vs[['icustay_id', 'intime', 'outtime', 'itemid', \n",
    "                                       'time', 'value', 'valuenum', 'valueuom']],\n",
    "                                 database=gluedatabase, \n",
    "                                 table='vs_windows',\n",
    "                                 dataset=True,\n",
    "                                 partition_cols=['itemid'],\n",
    "                                 path=f's3://{processed_data_bucket}/vs_windows')\n",
    "            if i % 200 == 0:\n",
    "                print(i)                \n",
    "                \n",
    "\n",
    "def get_vs_tokens(RELOAD=False):\n",
    "    # Load all distinct results per event type and return relevant decile information\n",
    "    query = f'select distinct(itemid), count(*) from {processed_db}.vs_windows group by itemid;'\n",
    "    processed_keys = pd.read_sql(query, connection)\n",
    "    processed_keys.columns=['itemid', 'c']\n",
    "    \n",
    "    distinct_vs = [v for v in processed_keys[processed_keys.c > 100].itemid]\n",
    "    \n",
    "    RELOAD = True\n",
    "    if RELOAD:\n",
    "        wr.s3.delete_objects(path=f's3://{processed_data_bucket}/vs_tokens')\n",
    "        done_keys = []\n",
    "    else:\n",
    "        query=f'select distinct(itemid) from {processed_db}.vs_tokens'\n",
    "        done_keys = list(pd.read_sql(query, connection).itemid)\n",
    "        \n",
    "    found_keys = [i for i in distinct_vs if i in done_keys]\n",
    "    to_find_keys = [int(i) for i in distinct_vs if not i in done_keys]\n",
    "    print(f'loading {len(found_keys)} keys from file and {len(to_find_keys)} keys from db')\n",
    "    \n",
    "    query = f'select subject_id, hadm_id, icustay_id from {gluedatabase}.icustays'\n",
    "    subj_stays = pd.read_sql(query, connection)\n",
    "    \n",
    "    # unit harmonisation for vital signs is slightly different than for test results,\n",
    "    # because there are a few tests with non-convertible units (i.e. cm-->kg) therefore instead of \n",
    "    # using a mapper, we split those tests into two sub-types, as they still contain useful\n",
    "    # numerical results.\n",
    "\n",
    "    units = {}\n",
    "\n",
    "    for i, item in enumerate(to_find_keys):\n",
    "        query = f'select icustay_id, intime, outtime, itemid, time, value, valuenum, valueuom from '\\\n",
    "                f'{processed_db}.vs_windows where itemid = \\'{item}\\''\n",
    "        item_range = pd.read_sql(query, connection)\n",
    "        item_range.valueuom = item_range.valueuom.str.lower()\n",
    "        try:\n",
    "            unit_count = item_range.valueuom.value_counts()\n",
    "            # if units are either x or '.' or 'none', assume all should be x\n",
    "            unit_list = [x for x in unit_count.index if x != 'none' and x != '.']\n",
    "            if len(unit_count) > 1: \n",
    "                print(f'splitting results for test {item}')\n",
    "                it_list = []\n",
    "                for j, unit in enumerate(unit_list):\n",
    "                    it = item_range[item_range.valueuom == unit].copy()\n",
    "                    it['decile'] = pd.qcut(it.valuenum, q=50, labels=list(range(50)), duplicates='drop')        \n",
    "                    it['decile_str'] = item_range.decile.astype('category').cat.codes.astype(str)\n",
    "                    it['label'] = it.itemid.astype(str) + '_' + str(j) + '_' + it.decile_str.astype(str)\n",
    "                    it_list.append(it)\n",
    "                item_range = pd.concat(it_list)\n",
    "            else:\n",
    "                item_range['decile'] = pd.qcut(item_range.valuenum, q=50, duplicates='drop')   \n",
    "                item_range['decile_str'] = item_range.decile.astype('category').cat.codes.astype(str)\n",
    "                item_range['label'] = item_range.itemid.astype(str) + '_' + item_range.decile_str.astype(str)\n",
    "        except:\n",
    "            item_range['label'] = item_range.value.astype('category').cat.codes.astype(str)\n",
    "        it = pd.merge(item_range[['icustay_id', 'time', 'label', 'itemid']], subj_stays, how='left', on='icustay_id')\n",
    "        \n",
    "        retain_labels = list(it.label.value_counts()[it.label.value_counts()>20].index)\n",
    "        d = it[it.label.isin(retain_labels)]\n",
    "        \n",
    "        if len(d) > 0:\n",
    "            wr.s3.to_parquet(df=d[['subject_id', 'hadm_id', 'time', 'label', 'itemid']], \n",
    "                                 database=gluedatabase, \n",
    "                                 table='vs_tokens',\n",
    "                                 dataset=True,\n",
    "                                 partition_cols=['itemid'],\n",
    "                                 path=f's3://{processed_data_bucket}/vs_tokens')\n",
    "        if i % 500 == 0:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proc_tokens(RELOAD=False):\n",
    "    if RELOAD:\n",
    "        wr.s3.delete_objects(path=f's3://{processed_data_bucket}/proc_tokens')\n",
    "        query = f'select subject_id, hadm_id, starttime, endtime, itemid, value, valueuom, storetime, ordercategoryname from {gluedatabase}.procedureevents_mv'\n",
    "        proc_df = pd.read_sql(query, connection)\n",
    "\n",
    "        timed_ranges = []\n",
    "        unit_mapper = {'day':1440, \n",
    "                       'hour':60, \n",
    "                       'min':1}\n",
    "        timed_categories = proc_df[(proc_df.valueuom == 'hour') | \n",
    "                                   (proc_df.valueuom == 'min') | \n",
    "                                   (proc_df.valueuom == 'day')].ordercategoryname.unique()\n",
    "        # convert all durations to minutes\n",
    "        for category in timed_categories:\n",
    "            timed_category = proc_df[proc_df.ordercategoryname == category].copy()\n",
    "            units = timed_category.valueuom.unique()\n",
    "            for unit in units:\n",
    "                if unit in unit_mapper.keys():\n",
    "                    multiplier = unit_mapper[unit]\n",
    "                    timed_category.loc[timed_category.valueuom == unit, 'value'] *= multiplier\n",
    "                    timed_category.loc[timed_category.valueuom == unit, 'valueuom'] = 'min'\n",
    "                else:\n",
    "                    raise ValueError(unit)\n",
    "            timed_category['quartile'] = pd.qcut(timed_category.value, 4, labels=list(range(4)))\n",
    "            timed_ranges.append(timed_category)\n",
    "\n",
    "        timed_tokens = []\n",
    "        for t in timed_ranges:\n",
    "            t['label'] = t.ordercategoryname + '_' + t.quartile.astype(str)\n",
    "            timed_tokens.append(t[['subject_id', 'hadm_id', 'storetime', 'label']])\n",
    "\n",
    "        all_timed_tokens = pd.concat(timed_tokens)\n",
    "        all_timed_tokens = all_timed_tokens.rename(columns={'storetime': 'time'})\n",
    "\n",
    "        # for untimed categories, we simply use the itemid at the time that it was stored in the database\n",
    "\n",
    "        untimed_categories = proc_df[(proc_df.valueuom != 'hour') &\n",
    "                                     (proc_df.valueuom != 'min') &\n",
    "                                     (proc_df.valueuom != 'day')].ordercategoryname.unique()\n",
    "\n",
    "        untimed_tokens = proc_df[proc_df.ordercategoryname.isin(untimed_categories)][['subject_id', 'hadm_id', 'storetime', 'itemid']].copy()\n",
    "        untimed_tokens.itemid = untimed_tokens.itemid.astype(str)\n",
    "        untimed_tokens = untimed_tokens.rename(columns={'storetime': 'time', 'itemid': 'label'})\n",
    "\n",
    "        proc_tokens = pd.concat([untimed_tokens, all_timed_tokens])\n",
    "\n",
    "        retain_labels = list(proc_tokens.label.value_counts()[proc_tokens.label.value_counts()>20].index)\n",
    "        d = proc_tokens[proc_tokens.label.isin(retain_labels)]\n",
    "        \n",
    "        wr.s3.to_parquet(df=d[['subject_id', 'hadm_id', 'time', 'label']], \n",
    "                         database=gluedatabase, \n",
    "                         table='proc_tokens',\n",
    "                         dataset=True,\n",
    "                         path=f's3://{processed_data_bucket}/proc_tokens')\n",
    "        \n",
    "    else:\n",
    "        proc_tokens = wr.s3.read_parquet_table(database=gluedatabase, table='proc_tokens')\n",
    "        \n",
    "    return proc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    get_vs_tokens(RELOAD=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnosis_splitter(diagnosis_tokens, to_split):\n",
    "    d = diagnosis_tokens['label'].str.split(to_split, expand=True)\n",
    "    diagnosis_tokens_split = pd.concat([diagnosis_tokens, d], axis=1)\n",
    "    split_list = []\n",
    "    for i in range(len(d.columns)):\n",
    "        split_diag = diagnosis_tokens_split[~diagnosis_tokens_split[i].isna()][['subject_id', 'hadm_id', 'time', i]].copy()\n",
    "        split_diag = split_diag.rename(columns = {i:'label'})\n",
    "        split_list.append(split_diag)\n",
    "    return pd.concat(split_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getage(admtime, dob):\n",
    "    age = over18(admtime, dob)\n",
    "    if age != None:\n",
    "        return f'age_{age}'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def over18(admtime, dob):\n",
    "    age = admtime.year - dob.year\n",
    "    if admtime.month < dob.month:\n",
    "        age -= 1\n",
    "    if age < 18:\n",
    "        return None\n",
    "    return age\n",
    "\n",
    "def get_demography_tokens(RELOAD=False):\n",
    "    if RELOAD:\n",
    "        wr.s3.delete_objects(path=f's3://{processed_data_bucket}/demographics_tokens')\n",
    "        query = f'SELECT p.subject_id, a.hadm_id, a.admittime, a.dischtime, a.diagnosis, p.gender, p.dob '\\\n",
    "                f'FROM {gluedatabase}.admissions a inner join {gluedatabase}.patients p on a.subject_id = p.subject_id;'\n",
    "        demography_df = pd.read_sql(query, connection)\n",
    "        demography_df['admission_age'] = demography_df.apply(lambda x: getage(x.admittime, x.dob), axis=1)\n",
    "        # dropping patients < 18 years of age\n",
    "        demography_df = demography_df.dropna(axis=0, subset=['admission_age'])\n",
    "        admission_tokens = demography_df[['subject_id', 'hadm_id', 'admittime']].copy()\n",
    "        admission_tokens['label'] = 'admission'\n",
    "        admission_tokens = admission_tokens.rename(columns={'admittime':'time'})\n",
    "        discharge_tokens = demography_df[['subject_id', 'hadm_id', 'dischtime']].copy()\n",
    "        discharge_tokens['label'] = 'discharge'\n",
    "        discharge_tokens = discharge_tokens.rename(columns={'dischtime':'time'})\n",
    "        gender_tokens = demography_df[['subject_id', 'hadm_id', 'gender', 'admittime']].copy()\n",
    "        gender_tokens = gender_tokens.rename(columns={'admittime':'time', 'gender':'label'})\n",
    "        age_tokens = demography_df[['subject_id', 'hadm_id', 'admission_age', 'admittime']].copy()\n",
    "        age_tokens = age_tokens.rename(columns={'admittime':'time', 'admission_age':'label'})\n",
    "\n",
    "        diagnosis_tokens = demography_df[['subject_id', 'hadm_id', 'diagnosis', 'dischtime']].copy()\n",
    "        diagnosis_tokens = diagnosis_tokens.rename(columns={'diagnosis': 'label', 'dischtime': 'time'})\n",
    "        split1 = diagnosis_splitter(diagnosis_tokens, '\\\\')\n",
    "        split2 = diagnosis_splitter(diagnosis_tokens, ';')\n",
    "        diagnosis_tokens = pd.concat([split1, split2])\n",
    "    \n",
    "        # add 3 days to every diagnosis that is available, on the assumption that this reflects \n",
    "        # a realistic delay in diagnosis availability.\n",
    "        \n",
    "        diagnosis_tokens.time += np.timedelta64(3, 'D')\n",
    "        demographics_tokens = pd.concat([admission_tokens, discharge_tokens, gender_tokens, age_tokens, diagnosis_tokens], sort=False)\n",
    "        demographics_tokens = demographics_tokens.dropna(axis=0)\n",
    "        \n",
    "        retain_labels = list(demographics_tokens.label.value_counts()[demographics_tokens.label.value_counts()>20].index)\n",
    "        d = demographics_tokens[demographics_tokens.label.isin(retain_labels)]\n",
    "        \n",
    "        wr.s3.to_parquet(df=d[['subject_id', 'hadm_id', 'time', 'label']], \n",
    "                 database=gluedatabase, \n",
    "                 table='demographics_tokens',\n",
    "                 dataset=True,\n",
    "                 path=f's3://{processed_data_bucket}/demographics_tokens')\n",
    "    else:\n",
    "        demographics_tokens = wr.s3.read_parquet_table(database=gluedatabase, table='demographics_tokens')\n",
    "    return demographics_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vent_tokens(RELOAD=False):\n",
    "    if RELOAD:\n",
    "        wr.s3.delete_objects(path=f's3://{processed_data_bucket}/vent_tokens')\n",
    "\n",
    "        query = f'select i.subject_id, i.hadm_id, i.icustay_id, '\\\n",
    "                f'v.ventnum, v.starttime, v.endtime, v.duration_hours '\\\n",
    "                f'from {gluedatabase}.icustays as i inner join '\\\n",
    "                f'{gluedatabase}.ventdurations as v on i.icustay_id = v.icustay_id'\n",
    "        vent_df = pd.read_sql(query, connection)\n",
    "\n",
    "        start_vent = vent_df[['subject_id', 'hadm_id', 'starttime']].copy()\n",
    "        start_vent['label'] = 'start_vent'\n",
    "        end_vent = vent_df[['subject_id', 'hadm_id', 'endtime']].copy()\n",
    "        end_vent['label'] = 'end_vent'\n",
    "\n",
    "        vent_df['dur_quant'] = pd.qcut(vent_df.duration_hours, 10, labels=list(range(10)))\n",
    "        vent_df['label'] = 'ventdur_' + vent_df.dur_quant.astype(str)\n",
    "\n",
    "        vent_df = vent_df[['subject_id', 'hadm_id', 'endtime', 'label']]\n",
    "        vent_df = vent_df.rename(columns={'endtime': 'time'})\n",
    "        start_vent = start_vent.rename(columns={'starttime': 'time'})\n",
    "        end_vent = end_vent.rename(columns={'endtime': 'time'})\n",
    "\n",
    "        vent_df = pd.concat([vent_df, start_vent, end_vent])\n",
    "    \n",
    "        retain_labels = list(vent_df.label.value_counts()[vent_df.label.value_counts()>20].index)\n",
    "        d = vent_df[vent_df.label.isin(retain_labels)]\n",
    "    \n",
    "        wr.s3.to_parquet(df=d, \n",
    "                         database=gluedatabase, \n",
    "                         table='vent_tokens',\n",
    "                         dataset=True,\n",
    "                         path=f's3://{processed_data_bucket}/vent_tokens')\n",
    "    else:\n",
    "        vent_df = wr.s3.read_parquet_table(database=gluedatabase, table='vent_tokens')\n",
    "    return vent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_med_tokens(RELOAD=False):\n",
    "    if RELOAD:\n",
    "        wr.s3.delete_objects(path=f's3://{processed_data_bucket}/med_tokens')\n",
    "        query = f'select subject_id, hadm_id, startdate, formulary_drug_cd, form_val_disp from {gluedatabase}.prescriptions'\n",
    "        med_tokens = pd.read_sql(query, connection)\n",
    "        med_tokens['label'] = med_tokens.formulary_drug_cd + '-' + med_tokens.form_val_disp\n",
    "        med_tokens = med_tokens.rename(columns={'startdate':'time'})\n",
    "        med_tokens = med_tokens[['subject_id', 'hadm_id', 'time', 'label']]\n",
    "        \n",
    "        retain_labels = list(med_tokens.label.value_counts()[med_tokens.label.value_counts()>20].index)\n",
    "        d = med_tokens[med_tokens.label.isin(retain_labels)]\n",
    "        \n",
    "        wr.s3.to_parquet(df=d[['subject_id', 'hadm_id', 'time', 'label']], \n",
    "                 database=gluedatabase, \n",
    "                 table='med_tokens',\n",
    "                 dataset=True,\n",
    "                 path=f's3://{processed_data_bucket}/med_tokens')\n",
    "    else:\n",
    "        med_tokens = wr.s3.read_parquet_table(database=gluedatabase, table='med_tokens')\n",
    "    return med_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_new_ICU_adm(row):\n",
    "    if (not isinstance(row.icustay_id, np.float)):\n",
    "        return False\n",
    "    if (row.curr_careunit != None):\n",
    "        if ('ICU' in row.curr_careunit):\n",
    "            return (row.prev_careunit == None) or (not ('ICU' in row.prev_careunit))\n",
    "        if ('CCU' in row.curr_careunit):\n",
    "            return (row.prev_careunit == None) or (not ('CCU' in row.prev_careunit))\n",
    "        if ('CSRU' in row.curr_careunit):\n",
    "            return (row.prev_careunit == None) or (not ('CSRU' in row.prev_careunit))\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_endpoints_df(RELOAD=False):\n",
    "    \n",
    "    # For simplicity, the prediction model considers only the first ICU admission \n",
    "    # of 6 or more hours duration for each hospital admission. \n",
    "\n",
    "    # 4 endpoints are considered:\n",
    "    #\n",
    "    # 1. at 6 hours after ICU admission, can we predict death within this ICU admission?\n",
    "    # 2. at 6 hours after ICU admission, can we predict death within this hospital admission?\n",
    "    # 3. at 6 hours after ICU admission, can we predict readmission to ICU within this hospital admission?\n",
    "    # 4. at 6 hours after ICU admission, can we predict icu duration > 48hr?\n",
    "\n",
    "    if RELOAD:\n",
    "        wr.s3.delete_objects(path=f's3://{processed_data_bucket}/endpoint_df')\n",
    "        query = f'SELECT a.subject_id, a.hadm_id, a.hospital_expire_flag, a.admittime, a.dischtime,'\\\n",
    "                f' p.dob, a.deathtime, i.intime, i.outtime FROM {gluedatabase}.admissions '\\\n",
    "                f' a INNER JOIN {gluedatabase}.icustays i'\\\n",
    "                f' ON a.hadm_id = i.hadm_id INNER JOIN {gluedatabase}.patients p on a.subject_id = p.subject_id'\n",
    "\n",
    "        endpoint_df = pd.read_sql(query, connection)\n",
    "        endpoint_df['duration'] = (endpoint_df.outtime - endpoint_df.intime)\n",
    "        endpoint_df = endpoint_df.dropna(axis=0, subset=['duration'])\n",
    "        endpoint_df['admission_age'] = endpoint_df.apply(lambda x: over18(x.admittime, x.dob), axis=1)\n",
    "        endpoint_df.duration = endpoint_df.duration.astype(int)\n",
    "        endpoint_df.hospital_expire_flag = endpoint_df.hospital_expire_flag.astype(int)\n",
    "        endpoint_df = endpoint_df.sort_values(['hadm_id', 'intime'])\n",
    "\n",
    "        wr.s3.to_parquet(df=endpoint_df, \n",
    "                         database=gluedatabase, \n",
    "                         table='endpoint_df',\n",
    "                         dataset=True,\n",
    "                         path=f's3://{processed_data_bucket}/endpoint_df')\n",
    "    else:\n",
    "        endpoint_df = wr.s3.read_parquet_table(database=gluedatabase, table='endpoint_df')\n",
    "    return endpoint_df\n",
    "\n",
    "\n",
    "def get_icu_adm(RELOAD=False):\n",
    "    \n",
    "    # We must load ICU admissions from the transfers_df so that we can account for ICU admissions on the same day.\n",
    "    # Note that in the original MIMIC data, any ICU readmissions on the same day are given the same ICUSTAY_ID.\n",
    "    # We combine all transfer rows where a patient is transferred, but remains within the ICU as a single\n",
    "    # ICU admission.  All transfer rows where a patient is transferred from ICU onto a general ward and then back \n",
    "    # to ICU are counted as a new ICU admission (ICU readmission).\n",
    "    \n",
    "    if RELOAD:\n",
    "        wr.s3.delete_objects(path=f's3://{processed_data_bucket}/icu_adm')\n",
    "        \n",
    "        query = f'select t.subject_id, t.hadm_id, t.icustay_id, t.eventtype, t.prev_careunit, t.curr_careunit,'\\\n",
    "                f' t.intime, t.outtime, t.los from {gluedatabase}.transfers t'\n",
    "        transfers_df = pd.read_sql(query, connection)\n",
    "\n",
    "        # locate all transfers that mark the start of a new ICU admission\n",
    "        transfers_df['new_admin'] = transfers_df.apply(lambda x: is_new_ICU_adm(x), axis=1)\n",
    "\n",
    "        # for all transfers that do not mark the start of a new ICU admin, but are within-ICU transfers, we \n",
    "        # must amalgamate the overall intime/outime to get the true full ICU admission\n",
    "        transfers_df = transfers_df[~transfers_df.icustay_id.isna()]\n",
    "        transfers_df = transfers_df.sort_values(['subject_id', 'hadm_id', 'intime'])\n",
    "\n",
    "        icu_list = []\n",
    "        new_id = 0\n",
    "\n",
    "        for idx, subject in enumerate(transfers_df.subject_id.unique()):\n",
    "            for hadm_id in transfers_df[transfers_df.subject_id==subject].hadm_id.unique():\n",
    "                transfers = transfers_df[transfers_df.hadm_id == hadm_id]\n",
    "                for icustay in transfers.icustay_id.unique():\n",
    "                    icurecords = transfers[(transfers.icustay_id == icustay) & ~(transfers.los.isna())]\n",
    "                    if len(icurecords[icurecords.new_admin]) > 0:\n",
    "                        intimes = list(icurecords[icurecords.new_admin].intime) + [max(icurecords.outtime)]\n",
    "\n",
    "                        for i, o in zip(intimes[:-1], intimes[1:]):\n",
    "                            full_icu_stay = icurecords[(icurecords.intime >= i) & (icurecords.intime < o)]\n",
    "                            new_id += 1\n",
    "                            try:\n",
    "                                icu_list.append({'subject_id': subject, \n",
    "                                                 'hadm_id': hadm_id, \n",
    "                                                 'icu_stay': icustay,\n",
    "                                                 'new_icu_stay': new_id,\n",
    "                                                 'intime': min(full_icu_stay.intime),\n",
    "                                                 'outtime': max(full_icu_stay.outtime)\n",
    "                                                })\n",
    "                            except ValueError:\n",
    "                                print(hadm_id, icustay)\n",
    "            if idx % 5000 == 0:\n",
    "                print(idx)\n",
    "        icu_admission_endpoints = pd.DataFrame(icu_list)\n",
    "\n",
    "        wr.s3.to_parquet(df=icu_admission_endpoints, \n",
    "                         database=gluedatabase, \n",
    "                         table='icu_adm',\n",
    "                         dataset=True,\n",
    "                         path=f's3://{processed_data_bucket}/icu_adm')\n",
    "    else:\n",
    "        icu_admission_endpoints = wr.s3.read_parquet_table(database=gluedatabase, table='icu_adm')\n",
    "    return icu_admission_endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_icu_tokens(full_endpoint_df, RELOAD=False):\n",
    "    if RELOAD:\n",
    "        wr.s3.delete_objects(path=f's3://{processed_data_bucket}/icu_tokens')\n",
    "        icu_admit_tokens = full_endpoint_df[['subject_id', 'hadm_id', 'intime']].copy()\n",
    "        icu_admit_tokens['label'] = 'icu_admit'\n",
    "        icu_admit_tokens = icu_admit_tokens.rename(columns={'intime': 'time'})\n",
    "        \n",
    "        # tokens stating this admission's ordinality for the subject in question are known \n",
    "        # at the time of admission to ICU\n",
    "        icu_ordinal_tokens = full_endpoint_df[['subject_id', 'hadm_id', 'intime', 'nth']].copy()\n",
    "        icu_ordinal_tokens['nth'] = 'icuadm_' + icu_ordinal_tokens['nth'].astype('str')\n",
    "        icu_ordinal_tokens = icu_ordinal_tokens.rename(columns={'intime': 'time', 'nth': 'label'})\n",
    "        \n",
    "        # duration tokens are only available at discharge\n",
    "        icu_dur_tokens = full_endpoint_df[['subject_id', 'hadm_id', 'outtime', 'duration']].copy()\n",
    "        icu_dur_tokens['duration'] = pd.qcut(icu_dur_tokens.duration, q=10, labels=list(range(10)))  \n",
    "        icu_dur_tokens['duration'] = 'icudur_' + icu_dur_tokens['duration'].astype('str')\n",
    "        icu_dur_tokens = icu_dur_tokens.rename(columns={'outtime': 'time', 'duration': 'label'})\n",
    "        icu_tokens = pd.concat([icu_admit_tokens, icu_ordinal_tokens, icu_dur_tokens]).sort_values(['subject_id', 'hadm_id', 'time'])\n",
    "\n",
    "        wr.s3.to_parquet(df=icu_tokens, \n",
    "                         database=gluedatabase, \n",
    "                         table='icu_tokens',\n",
    "                         dataset=True,\n",
    "                         path=f's3://{processed_data_bucket}/icu_tokens')\n",
    "        \n",
    "    else:\n",
    "        icu_tokens = wr.s3.read_parquet_table(database=gluedatabase, table='icu_tokens')\n",
    "    return icu_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_endpoints_df(icu_admission_endpoints, endpoint_df, RELOAD=False):\n",
    "    if RELOAD:\n",
    "        wr.s3.delete_objects(path=f's3://{processed_data_bucket}/icu_adm')\n",
    "        icu_admission_endpoints['duration'] = icu_admission_endpoints.outtime - icu_admission_endpoints.intime\n",
    "        print(f'data input contains {len(icu_admission_endpoints)} ICU admissions, across {len(icu_admission_endpoints.hadm_id.unique())} hospital admissions and {len(icu_admission_endpoints.subject_id.unique())} patients')\n",
    "\n",
    "        endpoint_df = endpoint_df.sort_values(['subject_id', 'hadm_id', 'intime'])\n",
    "\n",
    "\n",
    "        full_endpoint_df = pd.merge(icu_admission_endpoints[['hadm_id', 'new_icu_stay', 'intime', 'outtime', 'duration']], \n",
    "                                    endpoint_df[['subject_id','hadm_id','hospital_expire_flag','admittime',\n",
    "                                                 'dischtime','deathtime', 'admission_age']], \n",
    "                                    how='left', left_on=['hadm_id'], right_on=['hadm_id'])\n",
    "        \n",
    "        # Exclude rows that do not meet criteria:\n",
    "        # if the age at admissin is < 18 or \n",
    "        # if the first ICU stay in a hospital admission is either:\n",
    "        #              * shorter than 6 hours\n",
    "        #              * or the subject dies in the first 6 hours of ICU admission\n",
    "        # then that hospital admission is excluded from the model\n",
    "\n",
    "        # ordinality of this ICU admission within the current hospital admission\n",
    "        full_endpoint_df['nth'] = full_endpoint_df.groupby('hadm_id').cumcount()+1\n",
    "\n",
    "        excluded_hadm_id = full_endpoint_df[(full_endpoint_df.nth==1) &                           # first admission\n",
    "                                       ((full_endpoint_df.duration < np.timedelta64(6, 'h')) |    # and shorter than 6hr\n",
    "                                        (full_endpoint_df.admission_age < 18) |                   # or < 18yo\n",
    "                                        ((full_endpoint_df.hospital_expire_flag == 1) &           # or died and death time within 6hr\n",
    "                                         (full_endpoint_df.deathtime < full_endpoint_df.intime + np.timedelta64(6, 'h'))))].hadm_id.unique()\n",
    "\n",
    "        excluded = full_endpoint_df[full_endpoint_df.hadm_id.isin(excluded_hadm_id)]\n",
    "        included = full_endpoint_df[~full_endpoint_df.hadm_id.isin(excluded_hadm_id)]\n",
    "        \n",
    "        print(f'Excluded rows: {len(excluded)}, included rows: {len(included)}')\n",
    "        print(f'Excluded hospital admissions: {len(excluded.hadm_id.unique())}, hospital admissions: {len(included.hadm_id.unique())}')\n",
    "        print(f'Excluded patients: {len(excluded.subject_id.unique())}, included patients: {len(included.subject_id.unique())}')\n",
    "\n",
    "        full_endpoint_df = full_endpoint_df[~full_endpoint_df.hadm_id.isin(excluded_hadm_id)].copy()\n",
    "        full_endpoint_df = full_endpoint_df.sort_values(['subject_id', 'hadm_id', 'intime'])\n",
    "        full_endpoint_df.duration = full_endpoint_df.duration.astype(int)\n",
    "        full_endpoint_df.hospital_expire_flag = full_endpoint_df.hospital_expire_flag.fillna(0).astype(int)\n",
    "\n",
    "        full_endpoint_df = full_endpoint_df.dropna(axis=0, subset=['subject_id'])\n",
    "        full_endpoint_df.subject_id = full_endpoint_df.subject_id.astype(int)\n",
    "        \n",
    "        wr.s3.to_parquet(df=full_endpoint_df, \n",
    "                 database=gluedatabase, \n",
    "                 table='full_endpoints',\n",
    "                 dataset=True,\n",
    "                 path=f's3://{processed_data_bucket}/full_endpoints')\n",
    "    else:\n",
    "        full_endpoint_df = wr.s3.read_parquet_table(database=gluedatabase, table='full_endpoints')\n",
    "    return full_endpoint_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tokens(full_endpoint_df, RELOAD=False):\n",
    "\n",
    "    # This function grabs all of the tokenized data (words) of different clinical types \n",
    "    # and groups them into a single table to be turned into a patient trajectory (document) \n",
    "    # and be fed into the network\n",
    "    \n",
    "    wr.s3.delete_objects(path=f's3://{processed_data_bucket}/all_tokens')\n",
    "    pt = get_proc_tokens(RELOAD)\n",
    "    print(f'got {len(pt)} procedure tokens')\n",
    "    dt = get_demography_tokens(RELOAD)\n",
    "    print(f'got {len(dt)} demography tokens')\n",
    "    mt = get_med_tokens(RELOAD)\n",
    "    print(f'got {len(mt)} medication tokens')\n",
    "    vt = get_vent_tokens(RELOAD)\n",
    "    print(f'got {len(vt)} ventilation tokens')\n",
    "    \n",
    "    # Note that ICU tokens are NOT included in the patient trajectory, so that an embedding can be learned\n",
    "    # that does not leak data from any endpoints.  The ICU tokens will instead be summarised and fed into \n",
    "    # the network separately from the rest of the tokenized trajectory.\n",
    "    \n",
    "    # Due to the volume and frequency of the vital signs data, it is also treated separately to \n",
    "    # the other tokenized data types.\n",
    "    \n",
    "    ptht = wr.s3.read_parquet_table(database=gluedatabase, table='path_tokens')\n",
    "    print(f'got {len(ptht)} pathology tokens')\n",
    "    \n",
    "    for token_list in [pt, dt, mt, vt, ptht]:\n",
    "        wr.s3.to_parquet(df=token_list[['subject_id', 'hadm_id', 'time', 'label']], \n",
    "                         database=gluedatabase, \n",
    "                         table='all_tokens',\n",
    "                         dataset=True,\n",
    "                         path=f's3://{processed_data_bucket}/all_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_endpt_per_prediction(full_endpoint_df, RELOAD=False):\n",
    "    \n",
    "    # This function gets the selected prediction times and the associated relevant endpoints. \n",
    "    # Stores these in a table that can be used to build datasets.\n",
    "\n",
    "    if RELOAD:\n",
    "        wr.s3.delete_objects(path=f's3://{processed_data_bucket}/pred_endpt')\n",
    "    query = f'select distinct(subject_id, hadm_id) from {processed_db}.all_tokens'\n",
    "    subjects = pd.read_sql(query, connection)._col0.str.strip('{}field0=').str.split(', field1=', expand=True)\n",
    "    subjects.columns=['subject_id', 'hadm_id']\n",
    "    \n",
    "    subject_list = list(subjects.subject_id.unique())\n",
    "    print(f'loaded {len(subject_list)} subjects')\n",
    "    \n",
    "    six_hours = np.timedelta64(6, 'h')/np.timedelta64(1, 'ns')\n",
    "    seven_days = np.timedelta64(7, 'D')/np.timedelta64(1, 'ns')\n",
    "    \n",
    "    input_df_list = []\n",
    "    for i, subject in enumerate(subject_list):#token_split.subject_id.unique():\n",
    "        admission_list = subjects[subjects.subject_id == subject].hadm_id.unique()\n",
    "        for hadm_id in admission_list:\n",
    "            ef = full_endpoint_df[(full_endpoint_df.hadm_id == int(hadm_id)) & \n",
    "                                  (full_endpoint_df.duration > six_hours)].sort_values('intime')\n",
    "            # set prediction time as 6 hours after first ICU admission lasting at least 6 hours\n",
    "            if len(ef) > 0:\n",
    "                predict_time = (ef.intime + np.timedelta64(6, 'h')).iloc[0]\n",
    "                inhosp_death = ef.hospital_expire_flag.iloc[0] == 1\n",
    "                if inhosp_death:\n",
    "                    time_to_death = (ef.deathtime.iloc[0] - predict_time)/np.timedelta64(1, 'h')\n",
    "                else:\n",
    "                    time_to_death = -1\n",
    "                inicu_death = inhosp_death & (ef.deathtime.iloc[0] <= ef.outtime.iloc[0])\n",
    "                long_icu = ef.duration.iloc[0] >= seven_days\n",
    "                icu_len = ef.duration.iloc[0]\n",
    "                later_admissions = full_endpoint_df[(full_endpoint_df.hadm_id == hadm_id) & (full_endpoint_df.intime > predict_time)]\n",
    "                icu_readm = len(later_admissions) > 0\n",
    "                if icu_readm:\n",
    "                    time_to_readm = (later_admissions.sort_values('intime').iloc[0].intime - predict_time)/np.timedelta64(1, 'h')\n",
    "                else:\n",
    "                    time_to_readm = -1\n",
    "                input_df_list.append({'subj': subject, 'adm': hadm_id, \n",
    "                                      'time': predict_time,           # 6 hours after ICU admission time \n",
    "                                      'inhosp_death': inhosp_death,   # true if hospital expire flag is true for this hospital admission\n",
    "                                      'inicu_death': inicu_death,     # true if death time is before or the same as time of ICU discharge\n",
    "                                      'long_icu': long_icu,           # true if this ICU admission is longer than 48hr\n",
    "                                      'icu_readm': icu_readm,         # true if there is at least one ICU admission after the prediction target within this hospital admission\n",
    "                                      'time_to_death': time_to_death, # we aren't predicting time to death, but use this to weight augmentation strategy\n",
    "                                      'time_to_readm': time_to_readm, # as for time_to_death\n",
    "                                      'duration': icu_len,            # as for time_to_death\n",
    "                                      'predict_time': predict_time    # time that these prediction endpoints are valid for this subject\n",
    "                                     })\n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(f'prediction endpoints generated for {i} subjects')\n",
    "            data = pd.DataFrame(input_df_list)\n",
    "            wr.s3.to_parquet(df=data, database=gluedatabase, table=f'pred_endpt',\n",
    "                             dataset=True, path=f's3://{processed_data_bucket}/pred_endpt')\n",
    "            input_df_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vital signs pre-processing\n",
    "vs_todo = check_vs_to_process()\n",
    "make_vital_signs_windows(vs_todo)\n",
    "chart_event_ids = get_distinct_chart_events(RELOAD=False)\n",
    "# run the glue crawler after this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathology token pre-processing\n",
    "item_ids = get_distinct_lab_events(RELOAD=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_path_tokens(item_ids, RELOAD=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_vs_tokens(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all endpoiunts and tokens for ICU admissions and discharges\n",
    "endpoint_df = get_endpoints_df(True)\n",
    "icu_admission_endpoints = get_icu_adm(True)\n",
    "full_endpoint_df = get_full_endpoints_df(icu_admission_endpoints, endpoint_df, True)\n",
    "it = get_icu_tokens(full_endpoint_df, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process all clinical tokens other than vital signs\n",
    "join_tokens(full_endpoint_df, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the glue crawler before you get to this function\n",
    "get_endpt_per_prediction(full_endpoint_df, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
