{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models\n",
    "\n",
    "With this notebook, we train simple untunted recurrent models with sufficient performance to demonstrate the effect of data augmentation strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, pickle, boto3, sys\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow.keras.layers as tfkl\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_curve, average_precision_score, roc_curve, auc, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datagen import PROCESSED_DATAPATH, MODEL_INPUT_DATAPATH, RESULTFILE_DATAPATH, DataGen, strategies, targets, get_datafile\n",
    "from utils.utils import read_data, dump_data\n",
    "from utils.connections import model_input_bucket, processed_data_bucket, get_s3_keys_as_generator, download_file\n",
    "\n",
    "# check pre-reqs. \n",
    "assert(tf.__version__[0]=='2')\n",
    "assert len(tf.config.experimental.list_physical_devices('GPU')) > 0\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_features, output_dim, num_units, name, timesteps=500, dropout=0.3, unit_type='LSTM'):\n",
    "    model_input = tfk.Input(shape=(timesteps,), name=f'{name}_input')\n",
    "    x = tfkl.Embedding(max_features+1, output_dim, name=f'{name}_embed')(model_input)\n",
    "    if unit_type == 'LSTM':\n",
    "        x = tfkl.Bidirectional(tfkl.LSTM(units=num_units, name=f'{name}_recurrent',\n",
    "                                         return_sequences=False, \n",
    "                                         kernel_initializer='glorot_uniform', \n",
    "                                         bias_initializer='zeros'))(x)\n",
    "    elif unit_type == 'GRU':\n",
    "        x = tfkl.Bidirectional(tfkl.GRU(units=num_units, name=f'{name}_recurrent',\n",
    "                                        return_sequences=False, \n",
    "                                        kernel_initializer='glorot_uniform', \n",
    "                                        bias_initializer='zeros'))(x)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    x = tfkl.LeakyReLU(alpha=0.3)(x)\n",
    "    x = tfkl.Dropout(dropout)(x)\n",
    "    x = tfkl.Dense(num_units//2, activation='relu')(x)\n",
    "    x = tfkl.Dropout(dropout)(x)\n",
    "    model_output = tfkl.Dense(2, activation='softmax', name=f'{name}_output')(x)\n",
    "    return tfk.Model(inputs=model_input, outputs=model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def valid_epoch(data_valid, model, valid_step, data_index, valid_output=''):\n",
    "    valid_iter = iter(data_valid)\n",
    "    preds = []\n",
    "    ys = []\n",
    "    id_list = []\n",
    "    valid_loss = []\n",
    "    while True:\n",
    "        try:\n",
    "            x_clin, x_vs, y, ids, tt = next(valid_iter)\n",
    "            x = [x_clin, x_vs]\n",
    "            if data_index >= 0:\n",
    "                x = x[data_index]\n",
    "            pred, loss = valid_step(x, y, model)\n",
    "            valid_loss.append(loss.numpy())\n",
    "            preds.append(pred.numpy())\n",
    "            ys.append(y.numpy())\n",
    "            id_list.append(ids.numpy())\n",
    "        except:# StopIteration:\n",
    "            break\n",
    "    probabilities = np.vstack(preds)[:,0]\n",
    "    Y = np.hstack(ys)\n",
    "    IDS = np.hstack(id_list)\n",
    "    predictions = np.where(probabilities < 0.5, 0., 1.)\n",
    "    \n",
    "    if valid_output != '':\n",
    "        with open(valid_output, 'wb+') as outfile:\n",
    "            pickle.dump(probabilities, outfile)\n",
    "            pickle.dump(Y, outfile)\n",
    "            pickle.dump(IDS, outfile)\n",
    "    tp = len(np.where((predictions==Y)&(predictions==1))[0])\n",
    "    fp = len(np.where((predictions!=Y)&(predictions==1))[0])\n",
    "    tn = len(np.where((predictions==Y)&(predictions==0))[0])\n",
    "    fn = len(np.where((predictions!=Y)&(predictions==0))[0])\n",
    "    accuracy = (tp + tn)/len(predictions)\n",
    "    fpr, tpr, thresholds = roc_curve(Y, probabilities)\n",
    "    val_auc = auc(fpr, tpr)\n",
    "    sensitivity = tp/(tp + fn)\n",
    "    specificity = tn/(tn + fp)\n",
    "    try:\n",
    "        WDR = (tp + fp)/tp\n",
    "    except ZeroDivisionError:\n",
    "        WDR = 0\n",
    "    return np.mean(valid_loss), tp, fp, tn, fn, accuracy, val_auc, sensitivity, specificity, WDR\n",
    "\n",
    "def train_epoch(data_train, model, train_step, data_index):\n",
    "    t0 = time.clock()\n",
    "    train_iter = iter(data_train)\n",
    "    epoch_losses = []\n",
    "    while True:\n",
    "        try:\n",
    "            x_clin, x_vs, y, ids, tt = next(train_iter)\n",
    "            _, loss = train_step(x_clin, y, clin_model)\n",
    "            epoch_losses.append(loss.numpy())\n",
    "        except:# StopIteration:\n",
    "            break\n",
    "    t1 = time.clock() - t0\n",
    "    return t1, np.mean(epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_train, data_valid, model, data_index, epochs, label, epoch_start=0):\n",
    "    result_strings = []\n",
    "    @tf.function\n",
    "    def get_loss(Y, predictions):\n",
    "        return tfk.backend.binary_crossentropy(predictions[:,0], tf.cast(Y, tf.float32))\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(x, y, model):\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model(x, training=True)\n",
    "            loss = tf.reduce_mean(get_loss(y, preds))\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return preds, loss\n",
    "\n",
    "    @tf.function\n",
    "    def valid_step(x, y, model, training=False):\n",
    "        preds = model(x, training=training)\n",
    "        loss = tf.reduce_mean(get_loss(y, preds))\n",
    "        return preds, loss\n",
    "\n",
    "    runfile_name = datetime.now().strftime(\"%Y%m%d_%H:%M:%S\")\n",
    "    losses = []\n",
    "    min_loss = 1e20\n",
    "    for e in range(epoch_start, epochs):\n",
    "        elapsed_time, epoch_loss = train_epoch(data_train, model, train_step, data_index)\n",
    "        valid_output = f'{label}_{e}_preds'\n",
    "        val_loss, tp, fp, tn, fn, accuracy, val_auc, sensitivity, specificity, WDR = valid_epoch(data_valid, model, valid_step, data_index, valid_output)\n",
    "        result_string = (f'{label}\\t{e}\\t{elapsed_time}\\t{val_loss}\\t{epoch_loss}\\t{tp}\\t{fp}\\t{tn}\\t{fn}\\t{accuracy}\\t{val_auc}\\t{sensitivity}\\t{specificity}\\t{WDR}')\n",
    "        print(result_string)\n",
    "        result_strings.append(result_string)\n",
    "        model.save_weights(os.path.join(RESULTFILE_DATAPATH, f'{label}_e{e}.h5'))\n",
    "        with open(os.path.join(RESULTFILE_DATAPATH, 'summaries', f'{label}_{runfile_name}.tsv'), 'a+') as outfile:\n",
    "            outfile.write(result_string + '\\n')\n",
    "        if val_loss < min_loss:\n",
    "            min_loss = val_loss\n",
    "            losses = []\n",
    "        else:\n",
    "            losses.append(val_loss)\n",
    "        if len(losses)>3:\n",
    "            break\n",
    "    return result_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "\n",
    "max_features = {'clin': 7916, 'vs': 19405}\n",
    "embed_dim = {'clin': 150, 'vs': 100}\n",
    "data_index = {'clin': 0, 'vs': 1}\n",
    "widths = [5,10,15]\n",
    "name = 'clin'\n",
    "epochs = 100\n",
    "\n",
    "for fold in range(5):\n",
    "    for unit_type in ['GRU', 'LSTM']:\n",
    "        for target in ['hosp_death', 'icu_death', 'long_icu']:\n",
    "            for strategy in strategies:\n",
    "                data_train = get_datafile(target, strategy, fold=fold, \n",
    "                                          phase='train', batch_size=batch_size, \n",
    "                                          model_type='both')\n",
    "                data_valid = get_datafile(target, strategy, fold=fold, \n",
    "                                          phase='valid', batch_size=batch_size, \n",
    "                                          model_type='both')\n",
    "\n",
    "                for model_width in widths:\n",
    "                    clin_model = create_model(max_features[name], embed_dim[name], model_width, name, unit_type=unit_type)\n",
    "\n",
    "                    try:\n",
    "                        weight_files = [f for f in os.listdir(RESULTFILE_DATAPATH) if strategy in f and target in f and unit_type in f and f'width_{model_width}_fold_{fold}' in f and 'h5' in f]\n",
    "                        completed_epochs = max([int(w.split('.')[0].split('_')[-1].strip('e')) for w in weight_files])\n",
    "                        target_weight_file = f'{unit_type}_{strategy}_{target}_width_{model_width}_fold_{fold}_e{completed_epochs}.h5'\n",
    "                        print(target_weight_file, target_weight_file in weight_files)    \n",
    "                        clin_model.load_weights(os.path.join(RESULTFILE_DATAPATH, target_weight_file))\n",
    "                        print(completed_epochs)\n",
    "                    except (NameError, ValueError):\n",
    "                        weight_files = []\n",
    "                        completed_epochs = 0\n",
    "\n",
    "                    optimizer = tfk.optimizers.Adam(learning_rate=1e-4, clipnorm=1)\n",
    "                    clin_results = train_model(data_train, data_valid, clin_model, data_index[name], epochs, f'{unit_type}_{strategy}_{target}_width_{model_width}_fold_{fold}', completed_epochs + 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
