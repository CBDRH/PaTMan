{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models\n",
    "\n",
    "With this notebook, we train simple untunted recurrent models with sufficient performance to demonstrate the effect of data augmentation strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, pickle, boto3, sys\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow.keras.layers as tfkl\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_curve, average_precision_score, roc_curve, auc, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datagen import PROCESSED_DATAPATH, MODEL_INPUT_DATAPATH, RESULTFILE_DATAPATH, DataGen, strategies, targets, get_datafile\n",
    "from utils.utils import read_data, dump_data\n",
    "from utils.connections import model_input_bucket, processed_data_bucket, get_s3_keys_as_generator, download_file\n",
    "\n",
    "# check pre-reqs. \n",
    "assert(tf.__version__[0]=='2')\n",
    "assert len(tf.config.experimental.list_physical_devices('GPU')) > 0\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embedding_weight, name, timesteps=500, dropout=0.3):\n",
    "    max_features = embedding_weight.shape[0]\n",
    "    output_dim = embedding_weight.shape[1]\n",
    "    model_input = tfk.Input(shape=(timesteps,), name=f'{name}_input')\n",
    "    x = tfkl.Embedding(max_features+1, output_dim, name=f'{name}_embed')(model_input)\n",
    "    x = tfkl.Bidirectional(tfkl.LSTM(units=10, name=f'{name}_recurrent',\n",
    "                                     return_sequences=False, \n",
    "                                     kernel_initializer='glorot_uniform', \n",
    "                                     bias_initializer='zeros'))(x)\n",
    "    x = tfkl.LeakyReLU(alpha=0.3)(x)\n",
    "    x = tfkl.Dropout(dropout)(x)\n",
    "    x = tfkl.Dense(10, activation='relu')(x)\n",
    "    x = tfkl.Dropout(dropout)(x)\n",
    "    model_output = tfkl.Dense(2, activation='softmax', name=f'{name}_output')(x)\n",
    "    return tfk.Model(inputs=model_input, outputs=model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is definitely not the most efficient way of performing validation - the tfrecord\n",
    "# format is optimised for training, and for validation where no filtering is required.\n",
    "# It would have been better to not apply oversampling to the validation set for \n",
    "# oversample_basic and oversample_tte strategies, but it can be worked around.\n",
    "\n",
    "# For data augmentation strategies, we need to randomly filter the same number of \n",
    "# augmented samples for each ID before performing validation\n",
    "\n",
    "\n",
    "def valid_epoch(data_valid, model, valid_step, data_index):\n",
    "    valid_iter = iter(data_valid)\n",
    "    preds = []\n",
    "    ys = []\n",
    "    id_list = []\n",
    "    valid_loss = []\n",
    "    while True:\n",
    "        try:\n",
    "            x_clin, x_vs, y, ids, tt = next(valid_iter)\n",
    "            x = [x_clin, x_vs]\n",
    "            if data_index >= 0:\n",
    "                x = x[data_index]\n",
    "            pred, loss = valid_step(x, y, model)\n",
    "            valid_loss.append(loss.numpy())\n",
    "            preds.append(pred.numpy())\n",
    "            ys.append(y.numpy())\n",
    "            id_list.append(ids.numpy())\n",
    "        except StopIteration:\n",
    "            break\n",
    "    probabilities = np.vstack(preds)[:,0]\n",
    "    Y = np.hstack(ys)\n",
    "    IDS = np.hstack(id_list)\n",
    "    unique_filter = np.unique(IDS, return_index=True)[1]\n",
    "    probabilities = probabilities[unique_filter]\n",
    "    Y = Y[unique_filter]\n",
    "    predictions = np.where(probabilities < 0.5, 0., 1.)\n",
    "    tp = len(np.where((predictions==Y)&(predictions==1))[0])\n",
    "    fp = len(np.where((predictions!=Y)&(predictions==1))[0])\n",
    "    tn = len(np.where((predictions==Y)&(predictions==0))[0])\n",
    "    fn = len(np.where((predictions!=Y)&(predictions==0))[0])\n",
    "    accuracy = (tp + tn)/len(predictions)\n",
    "    fpr, tpr, thresholds = roc_curve(Y, probabilities)\n",
    "    val_auc = auc(fpr, tpr)\n",
    "    sensitivity = tp/(tp + fn)\n",
    "    specificity = tn/(tn + fp)\n",
    "    try:\n",
    "        WDR = (tp + fp)/tp\n",
    "    except ZeroDivisionError:\n",
    "        WDR = 0\n",
    "    return tp, fp, tn, fn, accuracy, val_auc, sensitivity, specificity, WDR\n",
    "\n",
    "def train_epoch(data_train, model, train_step, data_index):\n",
    "    t0 = time.clock()\n",
    "    train_iter = iter(data_train)\n",
    "    epoch_losses = []\n",
    "    while True:\n",
    "        try:\n",
    "            x_clin, x_vs, y, ids, tt = next(train_iter)\n",
    "            _, loss = train_step(x_clin, y, clin_model)\n",
    "            epoch_losses.append(loss.numpy())\n",
    "        except StopIteration:\n",
    "            break\n",
    "    t1 = time.clock() - t0\n",
    "    return t1, np.mean(epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_train, data_valid, model, data_index, epochs, label, epoch_start=0):\n",
    "    result_strings = []\n",
    "    @tf.function\n",
    "    def get_loss(Y, predictions):\n",
    "        return tfk.backend.binary_crossentropy(predictions[:,0], tf.cast(Y, tf.float32))\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(x, y, model):\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model(x, training=True)\n",
    "            loss = tf.reduce_mean(get_loss(y, preds))\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return preds, loss\n",
    "\n",
    "    @tf.function\n",
    "    def valid_step(x, y, model, training=False):\n",
    "        preds = model(x, training=training)\n",
    "        loss = tf.reduce_mean(get_loss(y, preds))\n",
    "        return preds, loss\n",
    "\n",
    "    runfile_name = datetime.now().strftime(\"%Y%m%d_%H:%M:%S\")\n",
    "    for e in range(epoch_start, epochs):\n",
    "        elapsed_time, epoch_loss = train_epoch(data_train, model, train_step, data_index)\n",
    "        tp, fp, tn, fn, accuracy, val_auc, sensitivity, specificity, WDR = valid_epoch(data_valid, model, valid_step, data_index)\n",
    "        result_string = (f'{label}\\t{e}\\t{elapsed_time}\\t{np.mean(epoch_loss)}\\t{tp}\\t{fp}\\t{tn}\\t{fn}\\t{accuracy}\\t{val_auc}\\t{sensitivity}\\t{specificity}\\t{WDR}')\n",
    "        print(result_string)\n",
    "        result_strings.append(result_string)\n",
    "        model.save_weights(os.path.join(RESULTFILE_DATAPATH, f'{label}_e{e}.h5'))\n",
    "        with open(os.path.join(RESULTFILE_DATAPATH, 'summaries', f'{label}_{runfile_name}.tsv'), 'a+') as outfile:\n",
    "            outfile.write(result_string + '\\n')\n",
    "    return result_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "batch_size=128\n",
    "\n",
    "for target in ['hosp_death', 'icu_death']:\n",
    "    for strategy in strategies:\n",
    "        if target != 'hosp_death' or strategy != 'original':\n",
    "            data_train = get_datafile(target, strategy, fold=fold, \n",
    "                                      phase='train', batch_size=batch_size, \n",
    "                                      model_type='both')\n",
    "            data_valid = get_datafile(target, strategy, fold=fold, \n",
    "                                      phase='valid', batch_size=batch_size, \n",
    "                                      model_type='both')\n",
    "\n",
    "            weight_files = [f for f in os.listdir(RESULTFILE_DATAPATH) if strategy in f and target in f and 'h5' in f]\n",
    "            completed_epochs = max([int(w.split('.')[0].split('_')[-1].strip('e')) for w in weight_files])\n",
    "            target_weight_file = f'{strategy}_{target}_e{completed_epochs}.h5'\n",
    "            clin_model = create_single_recurrent(clin_embedding_weight, 'clin')\n",
    "            clin_model.load_weights(os.path.join(RESULTFILE_DATAPATH, target_weight_file))\n",
    "\n",
    "            optimizer = tfk.optimizers.Adam(learning_rate=1e-4, clipnorm=1)\n",
    "\n",
    "            clin_results = train_model(data_train, data_valid, clin_model, 0, completed_epochs*3, f'{strategy}_{target}', completed_epochs + 1)\n",
    "            print('='*100)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
