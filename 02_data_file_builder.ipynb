{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model Input Data\n",
    "\n",
    "This file queries the tokenized trajectory tables (all_tokens, vs_tokens), endpoint table (pred_endpt) and ICU-specific input data table (icu_tokens) to create tensorflow record files that can be used to generate embeddings and final models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os, boto3, pickle, h5py, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from contextlib import ExitStack\n",
    "from itertools import product, repeat\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from IPython.display import display, HTML\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.connections import connection, cursor, gluedatabase, processed_db, upload_file, processed_data_bucket, download_file\n",
    "#from utils.datagen import PROCESSED_DATAPATH, MODEL_INPUT_DATAPATH, RESULTFILE_DATAPATH, strategies, targets, get_datafile\n",
    "from utils.datagen import PROCESSED_DATAPATH, MODEL_INPUT_DATAPATH\n",
    "from utils.utils import read_data, dump_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(table):\n",
    "    query = f'select label, count(*) from {processed_db}.{table} group by label'\n",
    "    vocab = pd.read_sql(query, connection)\n",
    "    \n",
    "    vocab_to_int = {v:i for i, v in enumerate(vocab[vocab._col1>=20].sort_values('_col1').label)}\n",
    "    int_to_vocab = {i:v for i, v in enumerate(vocab[vocab._col1>=20].sort_values('_col1').label)}\n",
    "    \n",
    "    rare = max(vocab_to_int.values()) + 1\n",
    "\n",
    "    def rare_str():\n",
    "        return 'rare'\n",
    "\n",
    "    def rare_int():\n",
    "        return rare\n",
    "    \n",
    "    i2v = defaultdict(rare_str, int_to_vocab)\n",
    "    v2i = defaultdict(rare_int, vocab_to_int)\n",
    "    return v2i, i2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_vocab():\n",
    "    vocabs = []\n",
    "    for table in ['all_tokens', 'vs_tokens']:\n",
    "        query = f'select label, count(*) from {processed_db}.{table} group by label'\n",
    "        vocabs.append(pd.read_sql(query, connection))\n",
    "    vocab = pd.concat(vocabs)\n",
    "    vocab_to_int = {v:i for i, v in enumerate(vocab[vocab._col1>=20].sort_values('_col1').label)}\n",
    "    int_to_vocab = {i:v for i, v in enumerate(vocab[vocab._col1>=20].sort_values('_col1').label)}\n",
    "    \n",
    "    rare = max(vocab_to_int.values()) + 1\n",
    "\n",
    "    def rare_str():\n",
    "        return 'rare'\n",
    "\n",
    "    def rare_int():\n",
    "        return rare\n",
    "    \n",
    "    i2v = defaultdict(rare_str, int_to_vocab)\n",
    "    v2i = defaultdict(rare_int, vocab_to_int)\n",
    "    return v2i, i2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _int_list_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _timestep_feature(value):\n",
    "    step_list = []\n",
    "    for v in value:\n",
    "        step_list.append(tf.train.Feature(int64_list=tf.train.Int64List(value=v)))\n",
    "    step_feature = tf.train.FeatureList(feature=step_list)\n",
    "    return tf.train.FeatureLists(feature_list={'data': step_feature})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def get_train_validation_splits(RELOAD=False):\n",
    "    \n",
    "    # we are splitting by subject, not admission, so as not to leak data across training and validation \n",
    "    # sets, although in this instance it's likely that would harm sensitivity rather than help, as for \n",
    "    # death endpoints, an earlier admission would have definitionally the opposite outcome for the \n",
    "    # overlapped data, rather than the same outcome.\n",
    "    \n",
    "    if RELOAD:\n",
    "        query = f'select distinct subject_id from {processed_db}.full_endpoints'\n",
    "        subjects = np.array(pd.read_sql(query, connection))\n",
    "        kf = KFold(n_splits=5, shuffle=True)\n",
    "        kfold_split = list(kf.split(subjects))\n",
    "        train_ids = [subjects[k[0]] for k in kfold_split]\n",
    "        valid_ids = [subjects[k[1]] for k in kfold_split]\n",
    "        with open(os.path.join(PROCESSED_DATAPATH, 'training_validation_splits'), 'wb') as outfile:\n",
    "            pickle.dump((train_ids, valid_ids), outfile)\n",
    "    else:\n",
    "        with open(os.path.join(PROCESSED_DATAPATH, 'training_validation_splits'), 'rb') as infile:\n",
    "            train_ids, valid_ids = pickle.load(infile)\n",
    "    return train_ids, valid_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_v2i, vs_i2v = get_full_vocab()\n",
    "clin_v2i, clin_i2v = get_vocabulary('all_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, valid_ids = get_train_validation_splits(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oversample_dist():\n",
    "    \n",
    "    # this function figures out the required multipliers to generate the correct \n",
    "    # oversample distributions (both simple oversample and time to event oversample)\n",
    "    # for all included endpoints\n",
    "\n",
    "    query = f'select * from {processed_db}.full_endpoints'\n",
    "    full_endpoint_df = pd.read_sql(query, connection)\n",
    "\n",
    "    six_hours = np.timedelta64(6, 'h')/np.timedelta64(1, 'ns')\n",
    "    seven_days = np.timedelta64(7, 'D')/np.timedelta64(1, 'ns')\n",
    "    long_enough = full_endpoint_df[(full_endpoint_df.nth==1)&\n",
    "                                   (full_endpoint_df.duration >= six_hours) & # duration is in ns in this df\n",
    "                                   (((full_endpoint_df.intime + np.timedelta64(6, 'h')) < full_endpoint_df.deathtime)\n",
    "                                    |(full_endpoint_df.hospital_expire_flag == 0))]\n",
    "    in_hosp_death_rate = len(long_enough[long_enough.hospital_expire_flag==1])/len(long_enough)\n",
    "    in_icu_death_rate = len(long_enough[(long_enough.hospital_expire_flag==1) & (long_enough.deathtime <= long_enough.outtime)])/len(long_enough)\n",
    "    long_icu_stay_rate = len(long_enough[long_enough.duration >= seven_days])/len(long_enough)\n",
    "    readmitted = full_endpoint_df[full_endpoint_df.hadm_id.isin(long_enough.hadm_id.unique()) & (full_endpoint_df.nth==2)]\n",
    "    readmitted_rate = len(readmitted)/len(long_enough)\n",
    "    print(f'Death in hospital: {in_hosp_death_rate}, Death in this ICU: {in_icu_death_rate}, Long ICU stay: {long_icu_stay_rate}, ICU readmission: {readmitted_rate}')\n",
    "\n",
    "    # we are targeting oversample to 50% of data, so what multiplier gets us to approximately\n",
    "    # that distribution for each endpoint?\n",
    "    \n",
    "    oversampling_rates = {'hosp_death': round(0.5/in_hosp_death_rate, 0), \n",
    "                          'icu_death': round(0.5/in_icu_death_rate, 0), \n",
    "                          'long_icu': round(0.5/long_icu_stay_rate, 0), \n",
    "                          'icu_readm': round(0.5/readmitted_rate, 0)}\n",
    "    print(oversampling_rates)\n",
    "\n",
    "    first_icu_adms = full_endpoint_df[full_endpoint_df.nth == 1].copy()\n",
    "    first_icu_adms['predtime'] = first_icu_adms.intime + np.timedelta64(6, 'h')\n",
    "\n",
    "    # get distribution of time to event values in hours for weighting strategy\n",
    "    death_times = (first_icu_adms.deathtime - first_icu_adms.predtime)[first_icu_adms.hospital_expire_flag == 1]/np.timedelta64(1, 'h')\n",
    "    in_icu_death_times = (first_icu_adms.deathtime - first_icu_adms.predtime)[(first_icu_adms.hospital_expire_flag == 1) & (first_icu_adms.deathtime <= first_icu_adms.outtime)]/np.timedelta64(1, 'h')\n",
    "\n",
    "    # distribution of admission time in weeks if > 1 week\n",
    "    duration_dist = np.histogram([d//seven_days if d//seven_days < 8 else 8 for d in first_icu_adms.duration], bins=9)[0][1:]\n",
    "\n",
    "    # getting time to readmission \n",
    "    full_endpoint_df = full_endpoint_df[['hadm_id', 'new_icu_stay', 'intime', 'outtime', 'duration',\n",
    "                                         'subject_id', 'hospital_expire_flag', 'admittime', 'dischtime',\n",
    "                                         'deathtime', 'admission_age']].drop_duplicates()\n",
    "\n",
    "    first_admissions = full_endpoint_df.sort_values('intime').groupby(['hadm_id']).first().reset_index()\n",
    "    subsequent = full_endpoint_df[~full_endpoint_df.new_icu_stay.isin(first_admissions.new_icu_stay)]\n",
    "\n",
    "    readmissions = subsequent[['hadm_id', 'intime']].sort_values('intime').groupby(['hadm_id']).first().reset_index()\n",
    "    readmissions = pd.merge(full_endpoint_df[['hadm_id','outtime']], readmissions, how='left')\n",
    "\n",
    "    full_endpoint_df['readm'] = np.where(full_endpoint_df.hadm_id.isin(subsequent.hadm_id), 1, 0)\n",
    "    full_endpoint_df['time_to_readm'] = np.where((full_endpoint_df.readm==1)&(full_endpoint_df.new_icu_stay.isin(first_admissions.new_icu_stay)), (readmissions.intime - readmissions.outtime)//np.timedelta64(24, 'h'), -1)\n",
    "\n",
    "    readmit_times = full_endpoint_df.time_to_readm\n",
    "    \n",
    "    # we make the assumption that once the time to event is more than 1 week in the future, the relationship between \n",
    "    # current patient status and endpoint time (not endpoint occurrance, just timing) becomes less deterministic\n",
    "    # so focus on over-weighting those subjects with the endpoint within 1 week most highly\n",
    "    \n",
    "    death_time_dist = np.histogram([d//24 if d <= 168 else 7 for d in death_times], bins=8)[0]\n",
    "    in_icu_death_time_dist = np.histogram([d//24 if d <= 168 else 7 for d in in_icu_death_times], bins=8)[0]\n",
    "    readmit_time_dist = np.histogram([d//24 if d <= 168 else 7 for d in readmit_times], bins=8)[0]\n",
    "\n",
    "    # targeting 5* oversampling for in-hosp death\n",
    "    target = 5*sum(death_time_dist)\n",
    "    in_hosp_multiplier = [9,8,7,6,5,4,3,2]\n",
    "    print(sum(in_hosp_multiplier*death_time_dist)/target)\n",
    "\n",
    "    # targeting 8* oversampling for in-ICU death\n",
    "    target = 8*sum(death_time_dist)\n",
    "    in_icu_multiplier = [16,15,14,13,12,11,10,9]\n",
    "    print(sum(in_icu_multiplier*in_icu_death_time_dist)/target)\n",
    "\n",
    "    # targeting 3* oversampling for long-ICU but there is no 'time-to' component, so we \n",
    "    # instead overweight by duration - oversampling the longest stays by the most\n",
    "    target = 3*sum(duration_dist)\n",
    "    long_stay_multiplier = [2,3,4,5,6,7,8,9]\n",
    "    print(sum(long_stay_multiplier*duration_dist)/target)\n",
    "\n",
    "    target = 8*sum(death_time_dist)\n",
    "    in_icu_multiplier = [16,15,14,13,12,11,10,9]\n",
    "    print(sum(in_icu_multiplier*in_icu_death_time_dist)/target)\n",
    "\n",
    "    # targeting 4* oversampling for readmission\n",
    "    target = 4*sum(readmit_time_dist)\n",
    "    readmit_multiplier = [4,3,3,3,3,3,3,3]\n",
    "    print(sum(readmit_multiplier*readmit_time_dist)/target)\n",
    "    \n",
    "    return oversampling_rates, in_hosp_multiplier, in_icu_multiplier, readmit_multiplier, long_stay_multiplier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampling_rates, in_hosp_multiplier, in_icu_multiplier, readmit_multiplier, long_stay_multiplier = get_oversample_dist()\n",
    "# dictionary of {endpoint : associated time weighting input} variables\n",
    "endpoint_lookup = {'hosp_death': 'tt_dth', 'icu_death': 'tt_dth', 'long_icu': 'duration', 'icu_readm': 'tt_readm'}\n",
    "oversampling_multipliers = {'hosp_death': in_hosp_multiplier, 'icu_death': in_icu_multiplier, 'icu_readm': readmit_multiplier, 'long_icu': long_stay_multiplier}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stepped_data(data, times):\n",
    "    # converts a serial list of events into steps of events that all occur\n",
    "    # within the same 1-hr span\n",
    "    \n",
    "    evt_list_stepped = []\n",
    "    timesteps = list(reversed(sorted(np.unique(times))))[-MAX_TIMESTEPS:]\n",
    "\n",
    "    for step in timesteps:\n",
    "        x_stepped = data[np.where(times==step)[0]][-TIMESTEP_WIDTH:]\n",
    "        evt_list_stepped.append(np.pad(x_stepped, (TIMESTEP_WIDTH-len(x_stepped), 0), 'constant'))\n",
    "    if len(evt_list_stepped) > 0:\n",
    "        return np.vstack(evt_list_stepped), timesteps\n",
    "    else:\n",
    "        return [], timesteps\n",
    "\n",
    "def augment(serial, stepped, aug_count):\n",
    "    \n",
    "    # randomly creates 10*aug_count augmentated versions of the input trajectory\n",
    "    # by truncating / masking / shuffling combinations of events within the list\n",
    "    try:\n",
    "        z = [shuffle_stepped(stepped, len(serial)) for _ in range(aug_count)]\n",
    "        augmentation_selector = np.random.choice([0, 1], 10*aug_count, p=[0.7, 0.3], replace=True)\n",
    "        first_data = min(np.nonzero(serial)[0])\n",
    "        data_elems = len(serial[first_data:])\n",
    "        x = [mask_serial(z[i//10], first_data, data_elems) for i, x in enumerate(augmentation_selector) if x == 0]\n",
    "        y = [truncate_serial(z[i//10], first_data, data_elems) for i, x in enumerate(augmentation_selector) if x == 1] \n",
    "        return x + y\n",
    "    except:\n",
    "        return [serial.astype(int) for _ in range(aug_count*20)]\n",
    "\n",
    "def mask_serial(serial, first_data, data_elems):\n",
    "    # removes somewhere between 1 and half the number of elements in the list - random mask\n",
    "    mask_num = np.random.randint(1, max(2, data_elems//2))\n",
    "    mask = np.random.choice(list(range(first_data, len(serial))), data_elems - mask_num, replace=False)\n",
    "    return np.hstack([[0]*(mask_num+first_data), serial[sorted(mask)]]).astype(np.int64)   \n",
    "\n",
    "def shuffle_stepped(stepped, n):\n",
    "    # shuffles events that occur within the same 1hr period and reassembles into a serial trajectory\n",
    "    try:\n",
    "        for i in range(stepped.shape[0]):\n",
    "            nz = np.nonzero(stepped[i])[0]\n",
    "            if len(nz) > 0:\n",
    "                first_data = min(nz)\n",
    "                np.random.shuffle(stepped[i][first_data:])\n",
    "        s = stepped[np.nonzero(stepped)].flatten()\n",
    "        return np.hstack([[0]*(max(0, n-len(s))), s[-n:]]).astype(np.int64)\n",
    "    except:\n",
    "        return np.array(stepped)\n",
    "\n",
    "def truncate_serial(serial, first_data, data_elems):\n",
    "    # removes somewhere between 1 and a third of the number of elements in the list - drops oldest events\n",
    "    truncate_num = np.random.randint(1, max(2, data_elems//3))\n",
    "    return np.hstack([[0]*(first_data+truncate_num), serial[first_data + truncate_num:]]).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(x, cat):\n",
    "    if cat=='clin':\n",
    "        return int(clin_v2i[x])\n",
    "    return int(vs_v2i[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(traj, predtime, cat):\n",
    "    \n",
    "    # gets portion of trajectory that was available for prediction at a given prediction time\n",
    "    \n",
    "    traj = traj[traj.time < predtime].sort_values('time')\n",
    "    serial_data = np.array([get_vocab(x, cat) for x in traj.label])\n",
    "    serial_times = np.array([round((predtime-t)/np.timedelta64(1, 'h'), 0) for t in traj.time])\n",
    "    data_stepped, times_stepped = get_stepped_data(serial_data, serial_times)\n",
    "    \n",
    "    serial_data = np.pad(serial_data[-SERIAL_TIMESTEPS:], (SERIAL_TIMESTEPS-len(serial_data[-SERIAL_TIMESTEPS:]), 0), mode='constant')\n",
    "    serial_times = np.pad(serial_times[-SERIAL_TIMESTEPS:], (SERIAL_TIMESTEPS-len(serial_times[-SERIAL_TIMESTEPS:]), 0), mode='constant')\n",
    "\n",
    "    return serial_data, serial_times, data_stepped, times_stepped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_dict(ct, vt, it, endpoints):\n",
    "    \n",
    "    # creates feature dictionary for tfrecord file from clincal trajectory, vital signs\n",
    "    # trajectory and endpoints at a given prediction time\n",
    "    \n",
    "    predtime = endpoints.predtime\n",
    "    clin_data, clin_times, clin_stepped_data, clin_stepped_times = get_data(ct, predtime, 'clin')\n",
    "    vs_data, vs_times, vs_stepped_data, vs_stepped_times = get_data(vt, predtime, 'vs')\n",
    "    hist_icu = len(it[(it.time<predtime)&(it.label.str.contains('admit'))])\n",
    "    durations = it[(it.time<predtime)&(it.label.str.contains('icudur'))]\n",
    "    try:\n",
    "        d = durations.label.str.split('_', expand=True)[1]\n",
    "        icu_ave_dur = np.mean(d)\n",
    "        icu_time_since = int((predtime - max(durations.time))//np.timedelta64(1, 'h'))\n",
    "    except:\n",
    "        icu_av_dur = -1\n",
    "        icu_time_since = -1\n",
    "\n",
    "    feature_dict = {'hosp_death': _int_feature(endpoints.hospital_expire_flag),\n",
    "                    'icu_death': _int_feature(endpoints.icu_death),\n",
    "                    'icu_readm': _int_feature(endpoints.readm),\n",
    "                    'long_icu': _int_feature(endpoints.long_icu), \n",
    "                    'subject': _int_feature(endpoints.subject_id),\n",
    "                    'hosp_adm': _int_feature(endpoints.hadm_id),\n",
    "                    'tt_dth': _int_feature(int(endpoints.time_to_death)),\n",
    "                    'tt_readm': _int_feature(int(endpoints.time_to_readm)),\n",
    "                    'duration': _int_feature(endpoints.duration_wk),\n",
    "                    'hist_icu': _int_feature(hist_icu),\n",
    "                    'ave_dur': _int_feature(icu_av_dur),\n",
    "                    'time_since': _int_feature(icu_time_since),\n",
    "                    'data_clin': tf.train.Feature(int64_list=tf.train.Int64List(value=clin_data.astype(int))),\n",
    "                    'times_clin': tf.train.Feature(int64_list=tf.train.Int64List(value=clin_times.astype(int))),\n",
    "                    'data_vs': tf.train.Feature(int64_list=tf.train.Int64List(value=vs_data.astype(int))),\n",
    "                    'times_vs': tf.train.Feature(int64_list=tf.train.Int64List(value=vs_times.astype(int)))}\n",
    "    \n",
    "    return feature_dict, clin_data, vs_data, clin_stepped_data, vs_stepped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_icu_endpt():\n",
    "    \n",
    "    # selects endpoints from database ready to combine with trajetories \n",
    "    # into tf record files\n",
    "    \n",
    "    query = f'select * from {processed_db}.full_endpoints'\n",
    "    full_endpoint_df = pd.read_sql(query, connection)\n",
    "\n",
    "    \n",
    "    full_endpoint_df = full_endpoint_df[['hadm_id', 'new_icu_stay', 'intime', 'outtime', 'duration',\n",
    "                                     'subject_id', 'hospital_expire_flag', 'admittime', 'dischtime',\n",
    "                                     'deathtime', 'admission_age']].drop_duplicates()\n",
    "\n",
    "    \n",
    "    query_icu = f'select * from {processed_db}.icu_tokens'\n",
    "    icu_traj = pd.read_sql(query_icu, connection)\n",
    "    \n",
    "    seven_days = np.timedelta64(7, 'D')/np.timedelta64(1, 'ns')\n",
    "    full_endpoint_df['predtime'] = full_endpoint_df.intime + np.timedelta64(6, 'h')\n",
    "    full_endpoint_df['long_icu'] = np.where(full_endpoint_df.duration >= seven_days, 1, 0)\n",
    "    full_endpoint_df['icu_death'] = np.where((full_endpoint_df.hospital_expire_flag==1)&(full_endpoint_df.deathtime <= full_endpoint_df.outtime), 1, 0)\n",
    "    \n",
    "    first_admissions = full_endpoint_df.sort_values('intime').groupby(['hadm_id']).first().reset_index()\n",
    "    subsequent = full_endpoint_df[~full_endpoint_df.new_icu_stay.isin(first_admissions.new_icu_stay)]\n",
    "\n",
    "    readmissions = subsequent[['hadm_id', 'intime']].sort_values('intime').groupby(['hadm_id']).first().reset_index()\n",
    "    readmissions = pd.merge(full_endpoint_df[['hadm_id','outtime']], readmissions, how='left')\n",
    "\n",
    "    full_endpoint_df['readm'] = np.where(full_endpoint_df.hadm_id.isin(subsequent.hadm_id), 1, 0)\n",
    "    full_endpoint_df['time_to_readm'] = np.where((full_endpoint_df.readm==1)&(full_endpoint_df.new_icu_stay.isin(first_admissions.new_icu_stay)), (readmissions.intime - readmissions.outtime)//np.timedelta64(24, 'h'), -1)\n",
    "\n",
    "    full_endpoint_df['time_to_death'] = np.where(full_endpoint_df.hospital_expire_flag==1, \n",
    "                                                 (full_endpoint_df.deathtime - full_endpoint_df.predtime)//np.timedelta64(24, 'h'), -1)\n",
    "    full_endpoint_df['duration_wk'] = full_endpoint_df.duration//(np.timedelta64(7, 'D')//np.timedelta64(1, 'ns'))-1\n",
    "    \n",
    "    return icu_traj, full_endpoint_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERIAL_TIMESTEPS = 500\n",
    "MAX_TIMESTEPS = 200\n",
    "TIMESTEP_WIDTH = 100\n",
    "import copy\n",
    "\n",
    "def serialize_data():\n",
    "    # combines data from db into basic feature_dict that is ready for data augmentation processes\n",
    "    offset = 500\n",
    "    icu_traj, full_endpoint_df = get_icu_endpt()\n",
    "    \n",
    "    subject_list = full_endpoint_df.subject_id.unique()\n",
    "    # batching data into managable chunks of length (offset) subjects\n",
    "    for i in range(0, len(subject_list), offset):\n",
    "        data_list = {}\n",
    "        subject_subset = tuple(subject_list[i:i+offset])\n",
    "        query_clin = f'select * from {processed_db}.all_tokens where subject_id in {subject_subset}'\n",
    "        query_vs = f'select * from {processed_db}.vs_tokens where subject_id in {subject_subset}'\n",
    "        clin_tokenized_traj = pd.read_sql(query_clin, connection)\n",
    "        vs_tokenized_traj = pd.read_sql(query_vs, connection)\n",
    "        print(f'selected data batch {i//offset} ({i} of {len(subject_list)})')\n",
    "        for j, admission in enumerate(clin_tokenized_traj.hadm_id.unique()):\n",
    "            ct = clin_tokenized_traj[clin_tokenized_traj.hadm_id == admission]\n",
    "            vt = vs_tokenized_traj[vs_tokenized_traj.hadm_id == admission]\n",
    "            it = icu_traj[icu_traj.hadm_id == admission]\n",
    "            endpoints = full_endpoint_df[full_endpoint_df.hadm_id == admission].sort_values('intime')\n",
    "            if len(endpoints)>0:\n",
    "                endpoints = endpoints.iloc[0]\n",
    "                data_list[admission] = (get_feature_dict(ct, pd.concat([ct, vt]), it, endpoints))\n",
    "        dump_data(os.path.join(PROCESSED_DATAPATH, f'serialized_{i}'), data_list)\n",
    "        \n",
    "def get_feature_value(feature, label):\n",
    "    # for convenience because tfrecord feature dictionaries are annoying\n",
    "    return feature[label].int64_list.value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiplier(value, target, strategy):\n",
    "    \n",
    "    # lookup appropriate multiplier according to endpoint value, endpoint target\n",
    "    # and weighting strategy\n",
    "    \n",
    "    if value < 0:\n",
    "        return 1\n",
    "    if strategy == 'basic':\n",
    "        return int(oversampling_rates[target])\n",
    "    lookup = min(value, len(oversampling_multipliers[target]) - 1)\n",
    "    return int(oversampling_multipliers[target][lookup])\n",
    "\n",
    "def do_oversampling(strategy):\n",
    "    \n",
    "    # strategy is either 'basic' or 'tte' - used to select the multiplier for positive\n",
    "    # class members - this can be either weighted to time to event or a single rate\n",
    "    # for all minority class\n",
    "    \n",
    "    serialized_filelist = [f for f in os.listdir(PROCESSED_DATAPATH) if 'serialized' in f]\n",
    "    for filename in serialized_filelist:\n",
    "        basic_data = read_data(os.path.join(PROCESSED_DATAPATH, filename))\n",
    "        #oversampled_data = {t:{'train':{}, 'valid':{}} for t in oversampling_rates.keys()}        \n",
    "        augmented_data = {t:{'train':{}, 'valid':{}} for t in oversampling_rates.keys()}\n",
    "\n",
    "        for admission in basic_data.keys():\n",
    "            feature_dict, clin_data, vs_data, clin_stepped_data, vs_stepped_data = basic_data[admission]\n",
    "\n",
    "            for target in oversampling_rates.keys():\n",
    "                # insert original data into the oversampled and augmented lists\n",
    "                #oversampled_data[target]['train'][admission] = [feature_dict]\n",
    "                #oversampled_data[target]['valid'][admission] = [feature_dict]\n",
    "\n",
    "                # what is the value of the target feature for this record?\n",
    "                value = get_feature_value(feature_dict, endpoint_lookup[target])\n",
    "                multiplier = get_multiplier(value, target, strategy)\n",
    "\n",
    "                #for _ in range(int(multiplier) - 1):\n",
    "                #    # add rate-1 more copies of this record to the oversampling list\n",
    "                #    oversampled_data[target]['train'][admission].append(feature_dict)\n",
    "\n",
    "                # make augmentation shuffles for both clinical and vital sign data\n",
    "                # note: we can augment for validation set and take average prediction as, \n",
    "                # final prediction value, but for validation we augment the same number\n",
    "                # of times, regardless of the feature value, whereas for the training\n",
    "                # set, we set the rate according to the multiplication factor as per\n",
    "                # oversampling strategy\n",
    "\n",
    "                clin_augmented_data = {'train': augment(clin_data, clin_stepped_data, multiplier),\n",
    "                                       'valid': augment(clin_data, clin_stepped_data, 10)}\n",
    "                vs_augmented_data = {'train': augment(vs_data, vs_stepped_data, multiplier),\n",
    "                                     'valid': augment(vs_data, vs_stepped_data, 10)}\n",
    "\n",
    "                for phase in ['train', 'valid']:\n",
    "                    augmented_data[target][phase][admission] = [feature_dict]\n",
    "                    for a, v in zip(clin_augmented_data[phase], vs_augmented_data[phase]):\n",
    "                        augmented_dict = copy.deepcopy(feature_dict)\n",
    "                        try:\n",
    "                            augmented_dict['data_clin'] = tf.train.Feature(int64_list=tf.train.Int64List(value=a))\n",
    "                            augmented_dict['data_vs'] = tf.train.Feature(int64_list=tf.train.Int64List(value=v))\n",
    "                            augmented_data[target][phase][admission].append(augmented_dict)\n",
    "                        except TypeError:\n",
    "                            # this will except if the augmented trajectory is a single event i.e. not iterable\n",
    "                            pass\n",
    "\n",
    "        suffix = filename.split('_')[1]\n",
    "        print(f'basic_rate_{suffix}')\n",
    "#        dump_data(os.path.join(PROCESSED_DATAPATH, f'{strategy}_rate_oversample_{suffix}'), oversampled_data)\n",
    "        dump_data(os.path.join(PROCESSED_DATAPATH, f'{strategy}_rate_augment_{suffix}'), augmented_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_files_weighted_distribution(weighting, strategy):\n",
    "    \n",
    "    # now that we have the data all in the right format and appropriately oversampled, combine\n",
    "    # them according to their k-fold assignment into the final tfrecord file that will be fed \n",
    "    # to the models\n",
    "    \n",
    "    for w in weighting:\n",
    "        for s in strategy:\n",
    "            print(w, s)\n",
    "            files = [f for f in os.listdir(PROCESSED_DATAPATH) if s in f and w in f]\n",
    "            train_file_names = [os.path.join(MODEL_INPUT_DATAPATH, f'train_{s}_{w}_{fold}') for fold in range(5)]\n",
    "            valid_file_names = [os.path.join(MODEL_INPUT_DATAPATH, f'valid_{s}_{w}_{fold}') for fold in range(5)]\n",
    "            with ExitStack() as stack:\n",
    "                train_files = {endpoint: [stack.enter_context(tf.io.TFRecordWriter(f'{t}_{endpoint}')) for t in train_file_names] for endpoint in endpoint_lookup.keys()}\n",
    "                valid_files = {endpoint: [stack.enter_context(tf.io.TFRecordWriter(f'{v}_{endpoint}')) for v in valid_file_names] for endpoint in endpoint_lookup.keys()}\n",
    "\n",
    "                for f in files:\n",
    "                    print(f)\n",
    "                    datafile = read_data(os.path.join(PROCESSED_DATAPATH, f))\n",
    "                    for target, target_data in datafile.items():\n",
    "                        for phase, phase_data in target_data.items():\n",
    "                            for visit, visit_data in phase_data.items():\n",
    "                                if len(phase_data) > 0:\n",
    "                                    for fold in range(5):\n",
    "                                        for d in visit_data:\n",
    "                                            subject = get_feature_value(d, 'subject')\n",
    "                                            traj_feat = tf.train.Features(feature=d)\n",
    "                                            example = tf.train.Example(features=traj_feat)\n",
    "                                            if (phase == 'train') and (subject in train_ids[fold]):                       \n",
    "                                                train_files[target][fold].write(example.SerializeToString())\n",
    "                                            elif (phase == 'valid') and (subject in valid_ids[fold]):\n",
    "                                                valid_files[target][fold].write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_files_original_distribution():\n",
    "    \n",
    "    # as per make_data_files_weighted_distribution but without weightings\n",
    "    \n",
    "    train_file_names = [os.path.join(MODEL_INPUT_DATAPATH, f'train_original_{fold}') for fold in range(5)]\n",
    "    valid_file_names = [os.path.join(MODEL_INPUT_DATAPATH, f'valid_original_{fold}') for fold in range(5)]\n",
    "    with ExitStack() as stack:\n",
    "        train_files = [stack.enter_context(tf.io.TFRecordWriter(t)) for t in train_file_names]\n",
    "        valid_files = [stack.enter_context(tf.io.TFRecordWriter(v)) for v in valid_file_names]\n",
    "        serialized_filelist = [f for f in os.listdir(PROCESSED_DATAPATH) if 'serialized' in f]\n",
    "        for filename in serialized_filelist:\n",
    "            print(filename)\n",
    "            basic_data = read_data(os.path.join(PROCESSED_DATAPATH, filename))\n",
    "            for admission in basic_data.keys():\n",
    "                feature_dict, clin_data, vs_data, clin_stepped_data, vs_stepped_data = basic_data[admission]\n",
    "                subject = get_feature_value(feature_dict, 'subject')\n",
    "                traj_feat = tf.train.Features(feature=feature_dict)\n",
    "                example = tf.train.Example(features=traj_feat)\n",
    "                for fold in range(5):\n",
    "                    if subject in train_ids[fold]:                        \n",
    "                        train_files[fold].write(example.SerializeToString())\n",
    "                    else:\n",
    "                        valid_files[fold].write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialize_data() # this step produces the endpoints tt_dth and tt_readm in full days and duration in full weeks\n",
    "make_data_files_original_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strategy in ['basic', 'tte']:\n",
    "    do_oversampling(strategy) # this step takes serialised data and produces augmented / oversampled data according to the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighting = ['tte', 'basic']\n",
    "strategy = ['augment', 'oversample']\n",
    "make_data_files_weighted_distribution(weighting, strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
